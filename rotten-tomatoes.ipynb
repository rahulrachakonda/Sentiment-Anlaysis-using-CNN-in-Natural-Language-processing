{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(color_codes=True)\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "Train= pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\n",
    "Test = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.217224144559786"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train['Phrase'].str.len().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train['Phrase'].str.len().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train['Sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Somewhat Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Somewhat Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "      <td>Somewhat Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "      <td>Somewhat Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId                                             Phrase  \\\n",
       "0           1           1  A series of escapades demonstrating the adage ...   \n",
       "63         64           2  This quiet , introspective and entertaining in...   \n",
       "81         82           3  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "116       117           4  A positively thrilling combination of ethnogra...   \n",
       "156       157           5  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "     Sentiment    sentiment_label  \n",
       "0            1  Somewhat Negative  \n",
       "63           4           Positive  \n",
       "81           1  Somewhat Negative  \n",
       "116          3  Somewhat Positive  \n",
       "156          1  Somewhat Negative  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create df of full sentences\n",
    "fullSent = Train.loc[Train.groupby('SentenceId')['PhraseId'].idxmin()]\n",
    "\n",
    "#Change sentiment to increase readability\n",
    "fullSent['sentiment_label'] = ''\n",
    "Sentiment_Label = ['Negative', 'Somewhat Negative', \n",
    "                  'Neutral', 'Somewhat Positive', 'Positive']\n",
    "for sent, label in enumerate(Sentiment_Label):\n",
    "    fullSent.loc[Train.Sentiment == sent, 'sentiment_label'] = label\n",
    "    \n",
    "fullSent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
       "                min_df=5, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['very', 'everyone', 'am', 'into', 'none', 'again',\n",
       "                            'several', 'least', 'therefore', 'also', 'it',\n",
       "                            'more', 'when', 'anything', 'up', 'becoming',\n",
       "                            'another', 'many', 'namely', 'all', 'always', 'do',\n",
       "                            'formerly', 'or', 'our', 'to', 'someone', 'whoever',\n",
       "                            'whom', 'beside', ...],\n",
       "                strip_accents='unicode', sublinear_tf=True,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add non-helpful stopwords to stopword list\n",
    "Stopwords = list(ENGLISH_STOP_WORDS)\n",
    "Stopwords.extend(['movie','movies','film','nt','rrb','lrb',\n",
    "                      'make','work','like','story','time','little'])\n",
    "\n",
    "#Create tfidf vectorizer object & fit to full sentence training data\n",
    "tfidf_vectorizor = TfidfVectorizer(min_df=5, \n",
    "                             max_df=0.5,\n",
    "                             analyzer='word',\n",
    "                             strip_accents='unicode',\n",
    "                             ngram_range=(1, 3),\n",
    "                             sublinear_tf=True, \n",
    "                             smooth_idf=True,\n",
    "                             use_idf=True,\n",
    "                             stop_words=Stopwords)\n",
    "\n",
    "tfidf_vectorizor.fit(list(fullSent['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.5, max_features=None, min_df=5,\n",
       "                ngram_range=(1, 3), preprocessor=None,\n",
       "                stop_words=['very', 'everyone', 'am', 'into', 'none', 'again',\n",
       "                            'several', 'least', 'therefore', 'also', 'it',\n",
       "                            'more', 'when', 'anything', 'up', 'becoming',\n",
       "                            'another', 'many', 'namely', 'all', 'always', 'do',\n",
       "                            'formerly', 'or', 'our', 'to', 'someone', 'whoever',\n",
       "                            'whom', 'beside', ...],\n",
       "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                 stop_words=Stopwords,\n",
    "                                 ngram_range=(1,3),\n",
    "                                 analyzer='word',\n",
    "                                 min_df=5,\n",
    "                                 max_df=0.5)\n",
    "\n",
    "BoW_vectorizer.fit(list(fullSent['Phrase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = np.array(Train['Phrase'])\n",
    "sentiment = np.array(Train['Sentiment'])\n",
    "# build train and test datasets\n",
    "phrase_train, phrase_test, sentiment_train, sentiment_test = train_test_split(phrase, \n",
    "                                                                              sentiment, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=4)\n",
    "\n",
    "#TF-IDF\n",
    "train_tfidfmatrix = tfidf_vectorizor.fit_transform(phrase_train)\n",
    "test_tfidfmatrix = tfidf_vectorizor.transform(phrase_test)\n",
    "\n",
    "#Vectorizer (Bag of Words Model)\n",
    "train_simplevector = BoW_vectorizer.transform(phrase_train)\n",
    "test_simplevector = BoW_vectorizer.transform(phrase_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(train, test, max_features, maxlen):\n",
    "    \n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "    train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n",
    "    test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n",
    "\n",
    "    X = train['Phrase']\n",
    "    test_X = test['Phrase']\n",
    "    Y = to_categorical(train['Sentiment'].values)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    return X, Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 125\n",
    "max_features = 10000\n",
    "\n",
    "X, Y, test_X = format_data(Train, Test, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1072,   63, 7286],\n",
       "       [   0,    0,    0, ...,   20, 2287,   23],\n",
       "       [   0,    0,    0, ..., 1969,    4, 2609],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   18,   27],\n",
       "       [   0,    0,    0, ..., 6545,  278, 3161],\n",
       "       [   0,    0,    0, ...,    0,    0, 8059]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  613, 1026,  393],\n",
       "       [   0,    0,    0, ...,  613, 1026,  393],\n",
       "       [   0,    0,    0, ...,    0,    0,   16],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    2,  126, 5835],\n",
       "       [   0,    0,    0, ...,    2,  126, 5835],\n",
       "       [   0,    0,    0, ...,    0,  373, 2008]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "      \n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        precision = true_positives / (possible_positives + K.epsilon())\n",
    "        return precision\n",
    "      \n",
    "        \n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, Dropout, Activation, GlobalMaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cnn_model(fea_matrix, n_class, mode, compiler):\n",
    "    model = Sequential()\n",
    "\n",
    "# Input / Embdedding\n",
    "    model.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "# CNN\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    if n_class == 1 and mode == \"cla\":\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        model.compile(optimizer=compiler, loss='binary_crossentropy',\n",
    "                  metrics=['acc', f1_m,precision_m,recall_m])\n",
    "    else:\n",
    "        #model.add(Activation('softmax'))\n",
    "        model.compile(optimizer=compiler, loss='categorical_crossentropy',\n",
    "                  metrics=['acc', f1_m,precision_m,recall_m])\n",
    "    return model\n",
    "# Output layer\n",
    "    #model.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "#batch_size = 32\n",
    "lr = 1e-4\n",
    "batch_size = 100\n",
    "#num_epochs = 10\n",
    "decay=1e-2\n",
    "mode = \"reg\"\n",
    "n_class = 3 #5\n",
    "\n",
    "adm = optimizers.Adam(lr = lr, decay = decay)\n",
    "sgd = optimizers.SGD(lr = lr, nesterov=True, momentum=0.7, decay=decay)\n",
    "Nadam = optimizers.Nadam(lr = lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model = baseline_cnn_model(X_train, n_class, mode, Nadam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117045 samples, validate on 39015 samples\n",
      "Epoch 1/100\n",
      "117045/117045 [==============================] - 15s 128us/step - loss: 1.5002 - acc: 0.5134 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 1.2143 - val_acc: 0.5210 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 1.1898 - acc: 0.5359 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 1.1340 - val_acc: 0.5563 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 1.0473 - acc: 0.5943 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 1.0861 - val_acc: 0.5872 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.9571 - acc: 0.6335 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 1.0245 - val_acc: 0.6249 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/100\n",
      "117045/117045 [==============================] - 11s 98us/step - loss: 0.9017 - acc: 0.6568 - f1_m: 1.7087e-05 - precision_m: 1.7087e-05 - recall_m: 1.7087e-05 - val_loss: 1.0338 - val_acc: 0.6309 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/100\n",
      "117045/117045 [==============================] - 12s 99us/step - loss: 0.8661 - acc: 0.6644 - f1_m: 1.7087e-05 - precision_m: 1.7087e-05 - recall_m: 1.7087e-05 - val_loss: 1.0527 - val_acc: 0.6409 - val_f1_m: 5.1262e-05 - val_precision_m: 5.1262e-05 - val_recall_m: 5.1262e-05\n",
      "Epoch 7/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.8355 - acc: 0.6761 - f1_m: 6.8349e-05 - precision_m: 6.8350e-05 - recall_m: 6.8350e-05 - val_loss: 1.0311 - val_acc: 0.6424 - val_f1_m: 7.6893e-05 - val_precision_m: 7.6894e-05 - val_recall_m: 7.6894e-05\n",
      "Epoch 8/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.8098 - acc: 0.6857 - f1_m: 2.5631e-05 - precision_m: 2.5631e-05 - recall_m: 2.5631e-05 - val_loss: 1.0995 - val_acc: 0.6444 - val_f1_m: 2.5631e-05 - val_precision_m: 2.5631e-05 - val_recall_m: 2.5631e-05\n",
      "Epoch 9/100\n",
      "117045/117045 [==============================] - 11s 98us/step - loss: 0.8083 - acc: 0.6835 - f1_m: 1.1107e-04 - precision_m: 1.1107e-04 - recall_m: 1.1107e-04 - val_loss: 1.0270 - val_acc: 0.6372 - val_f1_m: 3.5883e-04 - val_precision_m: 3.5884e-04 - val_recall_m: 3.5884e-04\n",
      "Epoch 10/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.7818 - acc: 0.6944 - f1_m: 2.2214e-04 - precision_m: 2.2214e-04 - recall_m: 2.2214e-04 - val_loss: 1.1917 - val_acc: 0.6467 - val_f1_m: 1.0252e-04 - val_precision_m: 1.0252e-04 - val_recall_m: 1.0252e-04\n",
      "Epoch 11/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.7759 - acc: 0.6980 - f1_m: 1.4524e-04 - precision_m: 1.4524e-04 - recall_m: 1.4524e-04 - val_loss: 1.0497 - val_acc: 0.6471 - val_f1_m: 1.0252e-04 - val_precision_m: 1.0252e-04 - val_recall_m: 1.0252e-04\n",
      "Epoch 12/100\n",
      "117045/117045 [==============================] - 11s 98us/step - loss: 0.7563 - acc: 0.6992 - f1_m: 3.7592e-04 - precision_m: 3.7592e-04 - recall_m: 3.7592e-04 - val_loss: 1.2917 - val_acc: 0.6488 - val_f1_m: 2.3068e-04 - val_precision_m: 2.3068e-04 - val_recall_m: 2.3068e-04\n",
      "Epoch 13/100\n",
      "117045/117045 [==============================] - 12s 98us/step - loss: 0.7359 - acc: 0.7084 - f1_m: 4.1864e-04 - precision_m: 4.1864e-04 - recall_m: 4.1864e-04 - val_loss: 1.1894 - val_acc: 0.6492 - val_f1_m: 1.7942e-04 - val_precision_m: 1.7942e-04 - val_recall_m: 1.7942e-04\n",
      "Epoch 14/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.7416 - acc: 0.7054 - f1_m: 7.6893e-04 - precision_m: 7.6894e-04 - recall_m: 7.6894e-04 - val_loss: 1.1329 - val_acc: 0.6377 - val_f1_m: 3.5883e-04 - val_precision_m: 3.5884e-04 - val_recall_m: 3.5884e-04\n",
      "Epoch 15/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.7166 - acc: 0.7126 - f1_m: 0.0012 - precision_m: 0.0012 - recall_m: 0.0012 - val_loss: 1.1299 - val_acc: 0.6267 - val_f1_m: 5.1262e-04 - val_precision_m: 5.1262e-04 - val_recall_m: 5.1262e-04\n",
      "Epoch 16/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.7078 - acc: 0.7201 - f1_m: 0.0018 - precision_m: 0.0018 - recall_m: 0.0018 - val_loss: 1.3196 - val_acc: 0.6500 - val_f1_m: 6.1514e-04 - val_precision_m: 6.1515e-04 - val_recall_m: 6.1515e-04\n",
      "Epoch 17/100\n",
      "117045/117045 [==============================] - 12s 99us/step - loss: 0.7000 - acc: 0.7231 - f1_m: 0.0016 - precision_m: 0.0016 - recall_m: 0.0016 - val_loss: 1.4475 - val_acc: 0.6513 - val_f1_m: 4.8699e-04 - val_precision_m: 4.8699e-04 - val_recall_m: 4.8699e-04\n",
      "Epoch 18/100\n",
      "117045/117045 [==============================] - 12s 99us/step - loss: 0.6822 - acc: 0.7338 - f1_m: 0.0010 - precision_m: 0.0010 - recall_m: 0.0010 - val_loss: 1.2974 - val_acc: 0.6522 - val_f1_m: 3.3320e-04 - val_precision_m: 3.3321e-04 - val_recall_m: 3.3321e-04\n",
      "Epoch 19/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.6889 - acc: 0.7344 - f1_m: 0.0013 - precision_m: 0.0013 - recall_m: 0.0013 - val_loss: 1.4122 - val_acc: 0.6439 - val_f1_m: 0.0015 - val_precision_m: 0.0015 - val_recall_m: 0.0015\n",
      "Epoch 20/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.6859 - acc: 0.7409 - f1_m: 0.0016 - precision_m: 0.0016 - recall_m: 0.0016 - val_loss: 1.3861 - val_acc: 0.6516 - val_f1_m: 0.0018 - val_precision_m: 0.0018 - val_recall_m: 0.0018\n",
      "Epoch 21/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.6542 - acc: 0.7457 - f1_m: 0.0018 - precision_m: 0.0018 - recall_m: 0.0018 - val_loss: 1.5524 - val_acc: 0.6530 - val_f1_m: 0.0010 - val_precision_m: 0.0010 - val_recall_m: 0.0010\n",
      "Epoch 22/100\n",
      "117045/117045 [==============================] - 12s 104us/step - loss: 0.6678 - acc: 0.7364 - f1_m: 0.0031 - precision_m: 0.0031 - recall_m: 0.0031 - val_loss: 1.5054 - val_acc: 0.6537 - val_f1_m: 0.0021 - val_precision_m: 0.0021 - val_recall_m: 0.0021\n",
      "Epoch 23/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.6429 - acc: 0.7490 - f1_m: 0.0029 - precision_m: 0.0029 - recall_m: 0.0029 - val_loss: 1.5318 - val_acc: 0.6511 - val_f1_m: 0.0024 - val_precision_m: 0.0024 - val_recall_m: 0.0024\n",
      "Epoch 24/100\n",
      "117045/117045 [==============================] - 11s 97us/step - loss: 0.6481 - acc: 0.7536 - f1_m: 0.0052 - precision_m: 0.0052 - recall_m: 0.0052 - val_loss: 1.3527 - val_acc: 0.6479 - val_f1_m: 0.0024 - val_precision_m: 0.0024 - val_recall_m: 0.0024\n",
      "Epoch 25/100\n",
      " 84300/117045 [====================>.........] - ETA: 2s - loss: 0.6368 - acc: 0.7630 - f1_m: 0.0034 - precision_m: 0.0034 - recall_m: 0.0034"
     ]
    }
   ],
   "source": [
    "#if n_class == 1 and mode == \"cla\":\n",
    "#        model.add(Activation('sigmoid'))\n",
    "#        model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "#                  metrics=['acc', f1_m,precision_m,recall_m])\n",
    "#else:\n",
    "#        model.add(Activation('softmax'))\n",
    "#        model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "#                  metrics=['acc', f1_m,precision_m,recall_m])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060/156060 [==============================] - 13s 82us/step\n",
      "Simple cnn model performance\n",
      "Accuracy:  0.8098\n",
      "Precision:  0.3697\n",
      "Recall:  0.3697\n",
      "F1 score:  0.3697\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(accuracy, f1_score, precision, recall):\n",
    "    print(\"Simple cnn model performance\")\n",
    "    print('Accuracy: ', np.round(accuracy, 4))\n",
    "    print('Precision: ', np.round(precision, 4))\n",
    "    print('Recall: ', np.round(recall, 4))\n",
    "    print('F1 score: ', np.round(f1_score, 4))\n",
    "    print('\\n')\n",
    "    \n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X, Y)\n",
    "print_metrics(accuracy, f1_score, precision, recall)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
